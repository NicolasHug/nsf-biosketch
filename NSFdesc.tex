
%%%%%%%%% PROPOSAL -- 15 pages (including Prior NSF Support)


% From the NSF Grants Proposal Guide:
% "The Project Description should provide a clear statement of the work 
% to be undertaken and must include: objectives for the period of the proposed 
% work and expected significance; relation to longer-term goals of the PI's 
% project; and relation to the present state of knowledge in the field, 
% to work in progress by the PI under other support and to work in progress 
% elsewhere."


\required{Project Description}
\setcounter{section}{0}
\section{Introduction}
As sciences move towards more data driven research, data analysis has
become a main building block of many research disciplines.
Many advances in recent research have been driven by algorithmic improvements
in data analysis, in particular in predictive analystics and machine learning.
[FIXME examples, cite?]
As machine learning becomes a tool for scientists accross disciplines, it is
important that this analytical tools are freely available, and easy to use for
domain scientists outside of the field of machine learning.

% FIXME fast prototyping

The scikit-learn machine learning library~\cite{pedregosa2011scikit} provides machine learning functunality
within the established --- but still growing --- scientific python ecosystem.
Scikit-learn is an open source library written in Python, implementing state-of-the-art
machine learning algorithm and utilities to apply these algorithms to real-world
data analysis and prediction problems.

The distinguishing features of scikit-learn are its generic and intuitive
interface, its comprehensiveness and its documentation~\cite{Varoquaux:2015:SML:2786984.2786995}.

\emph{Interface} Scikit-learn provides a generic interface for machine learning, mainly consisting
of only three methods: fit, to build models, predict, to make predictions using models,
and transform, to change the representation of the input data~\cite{buitinck2013api}.
This simple and consistent interface helps to abstract away the algorithm, and let
users focus on their particular problem. It also allows replacing an algorithm by another
by changing a single line of code.
Scikit-learn utilizes the well-established NumPy library to represent data and predictions.
NumPy is used across domains for numeric computations, and integrating this generic
representation into scikit-learn minimizes the friction of applying machine learning
within an existing project.

\emph{Comprehensiveness} Scikit-learn implements a wide variety of models for classification,
regression, clustering and dimensionality reduction, as well as methods for feature
selection and feature extraction. The library contains most of the algorithms included
in standard textbooks like FIXME bishop and ESL, while providing competitive implementation
of state-of-the-art algorithms like Gradient Boosting, Random Forests and SAG FIXME\@.
In addition to a large selection of algorithms, scikit-learn also contains a suite
of evaluation metrics and tools for parameter selection.

\emph{Documentation}
Documentation is a key ingredient to usability, and the documentation of scikit-learn
has been widely recognized as a useful resource FIXME\@. The scikit-learn project
has strict rules on documentation and requires examples and extensive descriptions
for all algorithms.

These features lead to a wide-spread use of scikit-learn, and the FIXME of a large
ecosystem of users, contributors, maintainers and dependent packages.

\section{Previous impact of the scikit-learn package}
The scikit-learn project has been widely used in academic and industrial research,
and has made its way into multiple commercial products. The
paper~\cite{pedregosa2011scikit} describing scikit-learn has been cited 2746
according to google scholar. Applications of scikit-learn spread
a multitude of research areas, including Physics, Astronomy,
Biology, Medicine, Psychology, Cyber Security, Oceanograpy, Sociology and more. FIXME add citations

The number of 2746 is likely underestimating the use of scikit-learn in published research, as a search for the
term ``scikit-learn'' yields 4850 results.
Other ways to measure the use and impact of scikit-learn is the engagement with
the project via code contributions, downloads, support requests and other
community interactions.
According to the email addresses used for code contributions (a conservative
estimate), 40 researchers and students from at least 23 US universities
\emph{contributed code} to scikit-learn, indicating wide-spread academic use.
These institutes include Berkeley, Brown, CMU, Columbia, Duke, Harvard, MIT,
NYU, Stanford, and University of Washington.
The total number of unique contributors to the project is about 700.

The scikit-learn mailing list has 1666 subscribers, including 70 email
addresses from ``edu'' domains, across 43 institutes.

On the question-answering site Stackoverflow, there are around 4500 questions
tagged as scikit-learn related, with 222 questions asked within the last 30
days of this writing.

For the last 5 years, the scientific Python conference scipy has had a scikit-learn tutorial,
showing the great demand for education in using machine learning and scikit-learn in particular.
In 2015, the tutorial was so overbooked that a second session was held.

Scikit-learn has become the center of the machine learning ecosystem in the scientific python community,
with several domain-specific packages relying on and extending its functionality.
Prominent examples of scientific software packages depending on scikit-learn
for machine learning include astroML~\cite{van2013openml} for astronomical
data, nilearn~\cite{abraham2014machine} for neuroimaging, MNE-Python for MEG
and EEG data, librosa~\cite{mcfee2015librosa} for audio and musical data,
nltk~\cite{bird2006nltk} and rosetta for Natural language processing, bcbio for
RNA sequence analysis, scikit-allel for genetic variation data, and rootpy for
the ROOT scientific software framework.
The main use of scikit-learn is outside of these major open source packages, though,
as stand alone library for data analysis. Using the github repository, we found
about 40.000 jupyter notebooks---a format for interactive computing with python---at the time of writing.

The scikit-learn website containing the documentation was visited by over
230.000 users in March 2016, upward from 200.000 users in February and 180.000 users in January,
reflecting the growth of the scikit-learn user community.

Several books have been written about the use of scikit-learn~\cite{garreta2013learning, hackeling2014mastering, hauck2014scikit, raschka2015python}.


\section{Proposed Work}
Despite the extensive documentation, applying machine learning using scikit-learn
still requires expert knowledge about:
what kind of models to use for a given task
what kind of feature extraction and preprocessing is required for a model
and what parameters to tune, and in which ranges

While it is easy for a scientist to create a working model, this model might not
be optimum, and they could obtain a much better model using more expert knowledge
in machine learning.
However, the need for this expert knowledge can be eliminated at least partially
using recent developments in model selection and meta-learning.

Funding form this proposal would enable a concentrated effort to include more
automatic model selection in scikit-learn, and therefore lower the barrier to
entry for applying machine learning even further.

The components of the proposed work are
- Improving the existing pipelining facilities for more automatic feature extraction.
- Provide explicit sets of parameters to tune for each algorithm, together with recommended ranges.
- Integrate methods for Bayesian optimization for parameter selection.
- Integrate meta-learning for automatic model selection.

\section{Intellectual Merit}
Improving automation in model selection and preprocessing in scikit-learn will have far-reaching
implications for existing and future applications of machine learning.
Research projects that already use scikit-learn for machine learning will be able to adapt better
models with minimal changes to their workflow, potentialy improving their research outcomes.
For research projects that are not relying on machine learning yet, including a model
from scikit-learn will be much easier and require much less expert knowledge.

Developing more automated model selection requires extensive benchmarking and evaluation
on a diverse array of machine learning problems. Such a large scale evaluation,
in the spirit of FIXME and autosklearn FIXME\@? can provide important insights into
the state of the art in machine learning, and will result in improved infrastructure
for large scale studies of algorithms.

\section{The scikit-learn package and ecosystem}
The scikit-learn machine learning library is an open source library for the
Python programming language, distributed under a BSD license.
It is developed largely by a community of volunteers, with some support from
Inria FIXME, Telecom ParisTech and through PI Mueller as part of the
Moore-Sloan Data Science Environment at NYU\@.
The package was first released in 2010, and new releases are made semi-annually.
The development team has 38 members, and releases and project management are
coordinated between PI Mueller and Olivier Grisel at FIXME\@.
There have been approximately 670 contributors to the project so far, demonstrating
the wide community engagement, with each release typically including changes
from 100 and 150 contributors.

\subsection{Project Description}
The scikit-learn project focusses on effective and easy to use implementation
of state-of-the-art machine learning algorithms that are usefull for a wide
audience of machine learning practitioners in research and commercial applications.
Implemented algorithms include Support Vector Machines, Random Forests, Gradient Boosting,
Non-Negative Matrix factorization, Independent Component Analysis, K-Means, DBSCAN, Isomap,
t-SNE and many others. To facilitate easy evaluation and model selection, scikit-learn
implements metrics like the the $R^2$, AUC, Adjusted Rand score, Mutual information, Average precision
and many more. Additionally, a framework for cross-validation and parameter selection is
provided, allowing parameter tuning with very little effort.

Given that scikit-learn is mostly developed and maintained by volunteers,
one of the core principles of scikit-learn is to lower the barrier for new developers,
and keep the complexity of the code as low as possible, to simplify maintenance.
The success of this approach can be seen in the large number of contributors to
the project.

\subsection{Interface and extensibility}
The simple interface of scikit-learn has received FIXME praise for its design and user-friendlyness.
The main functionality of machine learning models can be summarized using just three functions:

fit for building models

predict for creating predictions

transform for generating new representations.

Creating a custom model or preprocessing method using this interface is very simple,
and a way in which many users extend the functionality. The scikit-learn documentation
has comprehensive documentation on the conventions and interfaces in scikit-learn,
to promote the creation of custom extensions.
The scikit-learn library even has a generic test framework that allows users to
test their own implementation against the behavior expected by scikit-learn.

\subsection{Impacts in research}
Scikit-learn has had an impact first in the field of machine learning,
where it continues to provide a baseline for comparison, as well as a
framework in which to develop and evaluate new methods.
The much broader omni-disciplinary impact is in providing easy-to-use
tools for solving machine learning tasks. By providing a collection
of methods with a simple and consistent interface, researchers
can easily explore different solutions to their machine learning problems,
with a large collection of well-tested and well-established algorithms
at their finger tips.
The adoption of scikit-learn in research can be seen in the large number
of citations (over 2700 according to google scholar) as well as in the development
of more domain specific solutions build on scikit-learn.
According to the email addresses used for contributions, researchers and students
from at least 23 US universities \emph{contributed} to scikit-learn, indicating wide-spread use.

\subsection{Impacts in education}
In addition to being widely used as a toolkit by practitioners,
Scikit-learn is also popular in teaching machine learning.
The emphasis on accessibility, usability and documentation within
the scikit-learn project makes it ideal for an introductory
course in machine learning, and allows access to a wide variety
of algorithms. Scikit-learn is particularly popular in teaching
Data Science courses that focus on making inferences about
a particular data set, rather than the mathematics that go into
particular ways to solve machine learning problems.
Data Science courses are usually targeted at a broad spectrum
of students with mixed backgrounds, providing them
with the data analysis tools useful across domains.

Courses using scikit-learn have been tought at universities
including NYU, Brown, Duke, Berkeley, Stanford, Princeton,
Columbia, University of Pensilvania, Georgetown University, Cornell
and others. FIXME names spelling / naming.


\subsection{Reproducibility, democratization and openness}
An important contribution of scikit-learn has been in providing a common
ground for scientists to base their research on. Reimplementing
and algorithm from a text book or the description in a paper is often not
straight-forward, and slight differences in implementation details can
lead to different learning outcomes. By providing shared, open and 
accessible infrastructure that is used by many research groups,
scikit-learn facilitates reproducibility of algorithmic results.
The open source nature of scikit-learn means that everybody can have access
to advanced machine learning within the scientific python eco-system,
without having to spend money on commercial analysis software like Matlab,
SPSS or Stata.

\subsection{Integration in scientific python ecosystem}
Scikit-learn is firmly rooted in the scientific Python ecosystem, and has been
one of the catalysts of its success. Scikit-learn builds heavily on the
foundations on numpy and scipy, and integrates easily with the popular pandas
library for data analysis and the matplotlib library for plotting and
visualization.
Scikit-learn has been included in several scientific python distributions, such
as the popular cross-platform ContinuumIO Anaconda and Enthought Canopy
distributions, as well as the Python-xy distribution for Microsoft Windows.

The succinct interface of scikit-learn also lends itself well to interactive
data exploration and model building within the Jupyter Notebook environment.
As mentioned previously, searching for jupyter notebooks containing
scikit-learn code on the GitHub code hosting platform yields about 40000
results. FIXME

\section{Proposed Enhancements}
While the scikit-learn package is under constant development by a large community
of volunteers, the size and widespread use FIXME of the package result in much of this
time being spend on maintenance and usability improvement, leaving little room
for larger scale efforts to include major changes. The goal of this proposal
is to implement major usability and automation features in scikit-learn, decreasing
the domain expertise required to succesfully implement machine learning models.
We propose to improve three aspects of applying machine learning models that
currently require substantial expert knowledge: parameter selection, model
selection and data preprocessing.

\subsection{Default Parameter Ranges}
Nearly all machine learning models come with parameters to set or tune
to achieve good predictive performance. The most wide-spread way to adjust
these parameters is grid-search with nested cross-validation.
Grid-search describes the exhaustive search over all possible combinations
of the parameters under consideration.
A major hurdle in applying grid-search in practice is that algorithms
often have many different tuning parameters, making exhausive search
over all of them infeasible. However, in practice only a small subset
of the parameters is usually critical for good performance. This set,
and good candiate features are not usually well known and not included in text books.
The scikit-learn documentation tries to give guidelines, but these can be hard
to find and understand by people outside of machine learning.
We propose the inclusion of a programmatic way to query for the parameters
to adjust for each model, and what good parameter ranges are.

While there is some community consensus on this issue, we want to back
up our choices by large scale experiments on existing benchmark libraries
of datasets, like OpenML~\cite{van2013openml}.
\begin{description}
    \item [Task 1] Benchmark parameter ranges of commonly used supervised models.
    \item [Task 2] Provide default parameter ranges inside scikit-learn.
\end{description}

\subsection{Bayesian Optimization Based Parameter Selection}
An alternative to exhaustive grid-search in selecting parameters is using
bayesian optimization to iteratively improve the tuning parameters of a model~\cite{NIPS2011_4443, shahriari2016taking, NIP2012_4522}.
This technique has been well-established in the machine learning literature,
and there are several implementations available.~\cite{bergstra2013hyperopt, feurer-nips2015, komer2014hyperopt, snoek2015scalable}.
However, these algorithms have not made its way into the software used by
domain scientists that use machine machine learning algorithms as tools in
their research.
By incorporating a robust and efficient implementation into scikit-learn,
this proposal will bring the benefits of the recent advantages in model selection
from the machine learning community to the scientific users of scikit-learn.
\begin{description}
    \item [Task 3] Benchmark and integrate Gaussian Process based parameter oprimization.
    \item [Task 4] Benchmark Random Forest based parameter optimization.
    \item [Task 5] Integrate auto-sklearn bayesian optimization with scikit-learn.
\end{description}

\subsection{Automatic Preprocessing Selection}
Scikit-learn has a build-in mechanism to construct ``pipelines'' which are complex 
machine learning workflows, consisting of operations like feature extraction,
feature transformation, feature selection and predictive models.
All evaluation and parameter selection mechanisms in scikit-learn can operate
on these pipelines.
However, selecting which steps to chain together, that is which preprocessing
to use for which model, is left to the user. Currently there is no
automatic process to compare different pipeline constructions, even though
the right combination of methods is often not obvious in practice.
We propose to extend the model selection and pipelining framework in scikit-learn
to allow automatic selection of pipeline steps.
To facilitate automatic preprocessing selection, we will also programatically
define input and output conditions of different processing steps, such
as requirements for sparse data, dense data, non-negativity of features and others.
\begin{description}
    \item [Task 6] Allow setting of pipeline steps in scikit-learn parameter searches.
    \item [Task 7] Implement tagging of pre-conditions and post-conditions for data transformations.
    \item [Task 8] Refactor scikit-learn testing to validate pre-condition and post-condition tags.
    \item [Task 9] Collect common encoding schemes for categorical and
        continuous data in a scikit-learn compatible way.
\end{description}

\subsection{Meta-Learning}
Going beyond brute-force search or even bayesian optimization for parameter and
model selection, it is possible to use machine learning to recommend suitable
algorithms and parameters based on properties of a data set, such as number of
samples, number of features, number of classes and statistical properties of
the features~\cite{luo2015review, feurer-nips2015}.
Given the meta-features and the best pipeline and parameters found using
Bayesian optimization or grid-search, it is then possible to build a machine
learning model to predict what the best classifier for a new dataset would be.
This prediction based on meta-features is computationally much less demanding
than searching for a model and parameters from scratch for each new dataset.
Meta-learning allows the principled incorporation of expert knowledge as encoded
in the collection of datasets used for training.
While there are working implementations of meta-learning available (autoweka, auto-sklearn),
these projects are currently in a ``research software'' state. Making these methods
available to the wider scientific audience will require substantial engineering
effort. The above steps of incorporating Bayesian optimization and automatic
preprocessing selection will lay the foundation to enable meta-learning within the
scikit-learn framework.
\begin{description}
    \item [Task 10] Refactor auto-sklearn to make use of new pipeline and transformation conditions.
    \item [Task 11] Benchmark meta-learning features on OpenML datasets.
    \item [Task 12] Create meta-learning package with full user documentation
        and test coverage that is installable via the python package manager.
\end{description}

\section{Enabled Research Opportunities}
The proposed project will benefit researchers inside the machine learning field,
but more importantly will have an impact on new and existing applications of machine
learning across many domains of science.

\subsection{Reduce Barrier to Entry}
One of the premier goals of this project is to lower the barrier to entry
to applying machine learning in scientific applications even further.
Scikit-learn with its intuitive interface and comprehensive documentation
has made machine learning algorithms available to a much wider audience.
However, selecting and tuning models still requires machine learning expertise.
The wealth of algorithmic choices for solving a particular research problem can be
overwhelming to researchers from other disciplines. By builing more automated
abstractions on top of the existing machine learning algorithms in scikit-learn,
we will enable researchers to apply powerful models without learning
the characteristics and particularities of specific methods.
This will ease the adaption of machine learning for many researchers
that have not yet made use of machine learning.

\subsection{Improved Rapid Prototyping}
In data science, exploration is often limited by the human interactions needed to analyze data.
Being able to rapidly ask many research questions about a dataset or task of interest allows
quick exploration of hypotheses and speeds up research.
An often laborous and time-consuming part of analysis is data preprocessing and model tuning.
More automation in applying machine learning means that a researcher can ask a scientific
question in terms of a machine learning problem, start the automated machinery to
search for a model, and then continue exploring the data, without having to closely monitor
the process on the model. 
This frees up research time to investigate other questions, instead of trying to
find the right model to answer the first.

\subsection{Plug-In Replacements}
Many researchers are already using scikit-learn models in their projects, as witnessed
by the citations and other usage statistics we reported above. As the automation features
in this project will provide the same interface as the existing models, researchers
can simply replace the models in their existing projects by an automatic model search.
This will lead to better predictive results by simply changing two lines of code.

\subsection{Large-Scale comparisons}
Lastly, by providing a reproducible and open large-scale comparison of machine learning
methods on a wide varity of datasets, we provide guidance for future research
in machine learning itself.
In the tradition of FIXME and FIXME, we will identify strenghts and weaknesses of existing
models, to allow dissemination by the wider machine learning community.

\section{Community Engagement, Outreach, and Sustainability}
\subsection{Community integration}
As part of the core team and co-maintainer of scikit-learn developers, PI
Mueller is well integrated into the development process of scikit-learn.
This will enable the direct incorporation of many of the proposed enhancements
into the scikit-learn main package.
It will also provide a wide exposure of the proposed activities to the
scikit-learn community. We anticipate contributions to the proposed
project from the open source community from day 1 of this project.
The close connection of the PI Mueller to the scikit-learn user community will
also enable us to closely interact with users to ensure covering common use cases,
instead of creating software solutions in the vacuum.

\subsection{Software Quality and Testing}
The scikit-learn project has a history of high standards on code quality, reviews and testing.
Each new contribution to scikit-learn needs to be reviewed by at least two senior team members
in addition to the contributor. Often, many more reviews are performed. The
pull-requests (contributions) made to the project have on average 16 comments
made by developers and maintainers on improving code quality and algorithms.

Scikit-learn has an extensive testing suite, covering 94\% of lines of code in the project.
The test suite consists of unit tests, integration tests and algorithmic tests.
There is automated testing performed on all algorithms in scikit-learn that ensures a common
interface and user experience.
All tests are run as continuous integration tests on Microsoft Windows and Linux, and using
multiple versions of Python as well as multiple versions of numpy and scipy, ensuring
compatibility with a wide array of end-user systems.

Adding to the scikit-learn project, this proposal will leverage the existing infrastructure
and quality standards inside the scikit-learn community, ensuring high quality, well-tested code.

\subsection{Documentation and Distribution}
As for review processes and testing, the proposed project will be able to leverage the
well-tested documentation and distribution infrastructure of scikit-learn.
As mentioned above, there is continuous integration testing on multiple operating systems,
ensuring compatibility and seamless installation across platforms.
The integration sevices are also set up to build binary releases of the scikit-learn package
for distribution, so that making a release that can be installed on any platform is as
easy as tagging a commit as a release.
The continuous integration servers are also set up to rebuild the documentation on a per-change
basis, so that the documentation website (for the development version) is always up-to-date
with the current code.
As for testing, scikit-learn has a history of increasingly high standards for documentation,
requiring a description of algorithms, use cases, important parameters and theory,
as well as compelling examples. This culture of comprehensive and accessible documentation
will carry over to any additions made as part of this proposal.

\subsection{Sustainability Plan}
The scikit-learn community has grown substantially over the years, and volunteer efforts
are by far the biggest part of work contributed to the project.
While people do leave for personal reasons, often time commitments made as part
of more senior academic positions, the project has managed to smoothly integrate new
contributors. Due to substantial efforts to ease the barrier of entry, the scikit-learn
team is able to attract new volunteer core developers on a regular basis, and has
successfully transitioned from one ``generation'' to the next multiple times.
While there is an active community around scikit-learn, it is very hard to make
major changes based on volunteer efforts alone. New models that are added have often
been the product of internships or the Google Summer of Code program.
We therefore propose to hire a full time developer to greatly accelerate the progress
in terms of usability and automation.
Once these new features are included into the project, maintaining them via community
efforts will not require additional resources. FIXME this argument needs to be way better.

% GSOC somewhere? Broader impacts?

\section{Project Coordination and Evaluation Plan}
\subsection{Project Coordination and Timeline}
PI Mueller is a core developer and co-maintainer of scikit-learn and deeply integrated
into the community around it. The PI will lead the execution of the project and the integration
into scikit-learn and the scikit-learn ecosystem.
The developer will implement the new features and review changes proposed by
the scikit-learn community. The developer will also perform large-scale
benchmarking experiments to validate the effectiveness of the parameter
recommendations and the automation features.  The PI and developer will both
interact with the greater community via issue trackers, mailing lists and
chatrooms. The developer and PI will share working space to facilitate
collaboration.

FIXME timeline

\subsection{Need for a Senior Developer}
As many of the contributors of scikit-learn work in academic environments,
often the time they have available to do volunteer work on open source is
inversely proportional to their seniority.
Consequently, there are many junior contributors with good coding skills,
but less developed project management and timing skills.
Therefore it is paramount to have a senior developer to lead the efforts
on major new features, to ensure roadmap and scoping are useful and realistic.
Scikit-learn currently has around 430 open contributions (github pull requests)
that require code review and oversight from a senior developer to be integrated
into the project. This exemplifies the size of the community of contributors,
but also points out the bottle neck in terms of senior developers that
have enough machine learning and software development expertise to judge
the usefulness, efficiency and correctness of the proposed additions.


\subsection{Evaluation}
The success of an open source project is notoriously hard to measure. The
success and impact of an addition to an existing open source project even more
so. Open source software has many paths of distribution; in case of scikit-learn,
these are the python package manager (pypi), pre-packaged distributions like
anaconda, canopy and Python-xy, the package managers of linux distributions (like dpkg and rpm)
and downloads directly from the github repository. Most of these distributions paths are hard
or impossible to track. The python package manger for example reports over 500.000 downloads of the last
release, which is likely to be inflated when taken as a measure of users.
On the other hand, the number of citations of the relevant
paper\cite{pedregosa2011scikit}, around 2700, is likely to be much lower than
the number of papers actually using the scikit-learn library in their research.

To get a more comprehensive picture of the use FIXME of a package, we can look
at the citation and download counts together with other statistics, like the
number of discussions on the question answering site stackoverflow, the number
of contributors, the number of people writing to the mailing list, the number
of projects depending on the package etc.
These numbers are particularly meaningful when compared to other projects with
a similar scope.

We will develop the more novel features outside of the scikit-learn package, which will
facilitate measuring the impact of the proposed additions. The impact of contributions
to the scikit-learn package, however, is harder to measure. One simple measure of impact
is that some the proposed features will actually be integrated into scikit-learn.
Even though the PI is tied into the core developers, a contribution being accepted
is not guaranteed, and the guidelines on quality, usability and usefulness
are very high.

Another direct measure of impact is to count use of a particular function in
code available on github.com. As more and more research groups use version
control fore their experimental code, and publish it as open source, a growing
number of groups and individual researchers have a presence on github.com.
Counting the use of the added functionality, in particular inside Jupyter
Notebooks, provides great insight into the use of the proposed additions in
research and data analysis.


\section{Collaborations}
\subsection{Diversity in Open Source with WiMLDS}
While the scikit-learn community is quite successful in finding new contributors,
unfortunatly only one of the 38 scikit-learn core developers is a women.
Similar issues can be found in related packages, with numpy having no women among the 13 core
developers and scipy having one women out of 22 core developers.

To increase diversity, this proposal includes an annual workshop to increase
contributions to the open source community, in particular targeted at women.
To this end, we collaborate with the ``Women in Machine Learning and Data Science''
meetup group located in New York, a community of over 1500 (mostly) female data science and
machine learning experts.
The weekend-long workshop will give a short introduction to contributing to open source,
followed by two days of hands-on contributions to scikit-learn.
From prior experience, it is possible for new contributors to make meaningful changes
within one or two days, so that attendants will go home with a contribution
in the project. After pioneering the project with scikit-learn, our goal
it to engage with other open source projects with core developers local to New York
(pandas, matplotlib) to increase the impact of the workshop.

\subsection{Collaboration with auto-sklearn}
The auto-sklearn project~\cite{feurer-nips2015} currently provides a research
prototype of meta-learning with Bayesian optimization for parameter selection.
One of the main goals of this proposal is to transfer the research within the
auto-sklearn project to a easy-to-use and well-documented library, integrated
within the scikit-learn ecosystem. FIXME cites To this end, we will collaborate
with the auto-sklearn team, building upon their insights and technologies.
Frank Hutter commited to providing the support of his group to integrate our
additions to scikit-learn, such as the default parameter ranges and automated
preprocessing selection into their software, while in turn providing the
necessary domain knowledge to reproduce their research results in a
user-friendly library.

\subsection{Collaboration with OpenML}
Evaluation and benchmarking on real-world datasets are essential to this proposal, in providing
guidance for good parameter ranges, and assessing the success of automatic model selection methods.
The OpenML project~\cite{van2013openml} provides a quickly growing database of machine learning datasets with associated
tasks, including classification and regression~\cite{vanschoren2014openml}. FIXME number of datasets.
This growing database or research datasets will provide (FIXME) the basis of our assessments, and
ensure relevance of our effort across research domains.
Joaquin FIXMe commited to improving and maintaining the Python interface for the OpenML platform,
and improving the support for scikit-learn based models.

\subsection{Early Adopters}
The following researchers have commited to being \emph{early adopters} of our software
products. They will use the improvements and packages in their research projects,
and provide valuable feedback for ensuring usefulness of the software for a
variety of research applications:

Kyle Cranmer
David Hogg

\section{Broader Impacts of the Proposed Work}
The proposed project has a wide-reaching impact on the practical use of
machine learning in research, and on how machine learning can be taught to
domain experts.
Providing more automatic model selection will drastically lower the barrier
to entry to using machine learning for people without domain expertise
in machine learning.
Additionally, it will save time and effort spend by researchers doing
model selection by hand, replacing their effort by an automated process FIXME CITE\@.
This will make researchers more productive, and will allow them to focus
on their area of study.

Automation, coupled with publishing the results of large scale experiments will
also provide help in education. Currently, often parameters and
preprocessing are seen as undocumented expert knowledge, derived from personal experience.
Formalizing this knowledge, and providing a database of experiments,
will allow students to quickly master the necessary steps to apply
machine learning in practice.

The proposed project will also enable us to grow the open source community
within the data science and machine learning community.
In particular, the position of scikit-learn as a popular and valued
research library enables us to reach out to a broad community of users.
The prominent status of scikit-learn enables us to influence
the current make-up and values of the scientific open source ecosystem.
We proposed to host ``coding sprints'' to introduce
newcommers to contributing to open source and enlarging the
number of contributors even further. This event will be held collaboration with
the New York based Women in Machine Learning and Data Science group, to grow
the number of female contributors in particular. 

% as in the project summary, broader impacts must be called out separately 
% in the project description.  You may be able to give more specific
% examples, or discuss how you've previously achieved these impacts.
% It should be similar, but not identical, to the Broader Impacts statement
% in the project summary

\section{Results From Prior NSF Support}
Dr.\ Andreas Mueller has not been a PI or co-PI on NSF grants.
% 5 pages or fewer of the 15 pages for entire description document.
% include a summary of results of the completed work, 
% including accomplishments,supported from NSF grants received 
% in the past 5 years.
% if supported by more than one grant, choose the most relevant one
% for each grant, include: the title of the project, NSF award number, amount, dates of
% support, and publications resulting from this research.
% due to space limitations, it is often advisable to use citations rather
% than putting the titles of the publications in the body 
% of this section

% e.g.: "My prior grant, "Uses of Coffee in Mathematical Research" (DMS-0123456, 
% $100,000, 2005-2008), resulted in 3 papers [1],[2],[3], demonstrating..."

